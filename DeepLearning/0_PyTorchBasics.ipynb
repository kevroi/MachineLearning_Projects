{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.1 Tensors\n",
    "\n",
    "**Things that are good to know before starting**\n",
    "* Matrix Algebra\n",
    "* The python `numpy` library\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 0.1.1 What are tensors?\n",
    "\n",
    "A tensor can have lots of meanings, and I hate that it does. To a mathematician, \"A tensor is an object containing components which remains invariant no matter what cooridnate system you choose to describe it in because its coordinates change in a special, predictable way\" [1]. To a physicist, that definition boils down to a hand-wavy intuition: \"A tensor is anything that transforms like a tensor\". To a computer scientist, \"A tensor is a multidimensional array of numbers.\"\n",
    "\n",
    "For now, we'll stick with the computer science definition. They are exactly like `numpy` arrays, just with added functionality for Deep Learning (DL).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create a tensor of zeros:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import pytorch library\n",
    "import torch\n",
    "\n",
    "# create a tensor of zeros\n",
    "x = torch.zeros(20)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two things to note about this tensor: its shape and its contents. In this case we've set all it elements (a.k.a entries) to `0.0`. For our purposes, the above object is a *vector* or *rank 1 tensor*, since it is 1 d-imensional array or line of numbers.\n",
    "\n",
    "A scalar can be thought of as a rank 0 tensor, and rank 2 tensors are matrices or even greyscale images! This explanation is decent for a beginners intuition for DL, but will certainly anger mathematicians."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "identity matrix in 3D space: \n",
      " tensor([[1., 0., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 0., 1.]])\n",
      "an 8 by 8 matrix of noise: \n",
      " tensor([[0.5067, 0.6223, 0.5009, 0.5862, 0.2072, 0.8908, 0.5754, 0.7992],\n",
      "        [0.0761, 0.8800, 0.6511, 0.9567, 0.3908, 0.2077, 0.5643, 0.6257],\n",
      "        [0.9912, 0.0504, 0.4071, 0.1169, 0.2597, 0.9438, 0.9659, 0.6621],\n",
      "        [0.6483, 0.0144, 0.9687, 0.6128, 0.1858, 0.9572, 0.3776, 0.3475],\n",
      "        [0.8301, 0.6787, 0.1835, 0.0495, 0.0416, 0.3640, 0.1354, 0.0657],\n",
      "        [0.9910, 0.3281, 0.2735, 0.3247, 0.7514, 0.8350, 0.4247, 0.4262],\n",
      "        [0.9085, 0.3873, 0.6795, 0.4940, 0.3436, 0.6066, 0.5522, 0.8308],\n",
      "        [0.4434, 0.3847, 0.7489, 0.3145, 0.7269, 0.9336, 0.7203, 0.3894]])\n"
     ]
    }
   ],
   "source": [
    "# A familiar rank 2 tensor is the identity matrix\n",
    "I = torch.eye(3)\n",
    "print('Identity matrix in 3D space: \\n', I)\n",
    "noisy_tensor = torch.rand(8, 8)\n",
    "print('An 8 by 8 matrix of noise: \\n', noisy_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.1.2 Rank and Shape\n",
    "\n",
    "When generating `noisy_tensor` we had to specify its shape of 8 by 8.\n",
    "\n",
    "We use the term *shape* to describe the lengths and widths of these tensors. The shape of a tensor can be thought of as its dimensions and they tell us how many elements can be stored in the tensor, as well as the way they are arranged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of x is torch.Size([20])  and its rank is  1\n",
      "The shape of our identity is torch.Size([3, 3])  and its rank is  2\n",
      "The shape of this tensor: \n",
      " tensor([[[[0.5314, 0.0130],\n",
      "          [0.4840, 0.1022],\n",
      "          [0.9237, 0.8668]],\n",
      "\n",
      "         [[0.9909, 0.6661],\n",
      "          [0.9492, 0.3323],\n",
      "          [0.9356, 0.2287]]],\n",
      "\n",
      "\n",
      "        [[[0.5974, 0.1048],\n",
      "          [0.0860, 0.2611],\n",
      "          [0.7905, 0.7163]],\n",
      "\n",
      "         [[0.0832, 0.6680],\n",
      "          [0.3632, 0.3265],\n",
      "          [0.9020, 0.5058]]]]) \n",
      " is  torch.Size([2, 2, 3, 2]) \n",
      " while its rank is  4\n"
     ]
    }
   ],
   "source": [
    "x = torch.zeros(20)\n",
    "print('The shape of x is', x.shape, ' and its rank is ', len(x.shape))\n",
    "I = torch.eye(3)\n",
    "print('The shape of our identity is', I.shape, ' and its rank is ', len(I.shape))\n",
    "big_tensor = torch.rand(2, 2, 3, 2)\n",
    "print('The shape of this tensor: \\n', big_tensor, '\\n is ', big_tensor.shape, '\\n while its rank is ', len(big_tensor.shape))\n",
    "# notice how the shape is returned in a torch.Size object. We can index it to return specific dimensions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also increase or decrease the rabnk of a tensor by adding or removing extra dimensions to them using the `unsqueeze` and `squeeze` commands respectively. Note that these DO NOT add extra data into the tensor, they just wrap existing data in an extra pair of prackets (this seemingly useless additional dimension of size 1 is called the *singleton* dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a = tensor([1., 2., 3.])\n",
      "a has shape [3] and rank 1\n",
      "UNSQUEEZING\n",
      "a = tensor([[1., 2., 3.]])\n",
      "a has shape [1, 3] and rank 2\n",
      "SQUEEZING BACK\n",
      "a = tensor([1., 2., 3.])\n",
      "a has shape [3] and rank 1\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([1.0, 2.0, 3.0])\n",
    "print(f\"a = {a}\")\n",
    "print(f\"a has shape {list(a.shape)} and rank {len(a.shape)}\")\n",
    "\n",
    "a = a.unsqueeze(0) # add new dim to the 0th index\n",
    "print(\"UNSQUEEZING\")\n",
    "print(f\"a = {a}\")\n",
    "print(f\"a has shape {list(a.shape)} and rank {len(a.shape)}\")\n",
    "\n",
    "a.squeeze_(0) # remove new dim to the 0th index\n",
    "# adding underscore to the end of any method makes it inplace\n",
    "# so I dont need to let *new a* = squeeze(*old a*)\n",
    "# this saves having to write a copy to memory: useful for huge tensors!\n",
    "print(\"SQUEEZING BACK\")\n",
    "print(f\"a = {a}\")\n",
    "print(f\"a has shape {list(a.shape)} and rank {len(a.shape)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Singleton dimensions are automatically filled if the need arises during an operation, like addition - this breaks the rules of typical matrix algebra you learn about in school ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.8809, 0.9910, 0.2387]])\n",
      "+\n",
      "tensor([[0.4236, 0.0988, 0.3767],\n",
      "        [0.7340, 0.8531, 0.5838],\n",
      "        [0.3873, 0.0886, 0.0947]])\n",
      "=\n",
      "tensor([[1.3045, 1.0898, 0.6154],\n",
      "        [1.6150, 1.8441, 0.8224],\n",
      "        [1.2683, 1.0797, 0.3334]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand(1, 3)\n",
    "print(a)\n",
    "print('+')\n",
    "b = torch.rand(3, 3)\n",
    "print(b)\n",
    "print('=')\n",
    "print (a+b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can repeat the contents of a tensor along a dimension using the `repeat` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3., 4.])\n",
      "a has shape torch.Size([4])\n",
      "tensor([[1., 2., 3., 4., 1., 2., 3., 4., 1., 2., 3., 4.],\n",
      "        [1., 2., 3., 4., 1., 2., 3., 4., 1., 2., 3., 4.]])\n",
      "a now has shape torch.Size([2, 12])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([1.0, 2.0, 3.0, 4.0])\n",
    "print(a)\n",
    "print(f\"a has shape {a.shape}\")\n",
    "\n",
    "a = a.repeat(2, 3)\n",
    "# The best way to read the repeat method is by looking at its arguments backwards\n",
    "# here we repeat it thrice along the dim where the data is,\n",
    "# then repeat that dim twice\n",
    "print(a)\n",
    "print(f\"a now has shape {a.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can rearrange the contents of a tensor into a tensor of a different shape using the `view` method. The number of elements remain unchanged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As a 24 by 1 tensor: \n",
      " tensor([ 1.1540, -1.2635, -0.7792,  1.0439,  0.3957, -0.3714, -0.6501, -0.4618,\n",
      "        -1.1161, -1.8036, -3.5115,  0.0693,  0.7241,  0.3824, -0.2927,  0.3986,\n",
      "        -1.4391,  0.9888, -0.2673, -0.1706, -0.1322,  0.1269, -0.0269, -0.3146])\n",
      "As a 8 by 3 tensor: \n",
      " tensor([[ 1.1540, -1.2635, -0.7792],\n",
      "        [ 1.0439,  0.3957, -0.3714],\n",
      "        [-0.6501, -0.4618, -1.1161],\n",
      "        [-1.8036, -3.5115,  0.0693],\n",
      "        [ 0.7241,  0.3824, -0.2927],\n",
      "        [ 0.3986, -1.4391,  0.9888],\n",
      "        [-0.2673, -0.1706, -0.1322],\n",
      "        [ 0.1269, -0.0269, -0.3146]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3, 4, 2) # 3*4*2 = 24 elements in total within this tensor\n",
    "print(f\"As a 24 by 1 tensor: \\n {x.view(24)}\")\n",
    "print(f\"As a 8 by 3 tensor: \\n {x.view(8,3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are reshaping the tensor like this, you can make sure the representation of the tensor in memory respects this new shape using the `contiguous` method. This  makes a copy of the tensor where the order of its elements in memory is the same as if it had been created from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function Tensor.contiguous>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(3, 4, 2)\n",
    "x.view(3, 8).contiguous"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.1.3 Indexing\n",
    "Indexing lets you access subsets of the data stored in tensors, and tensors can be indexed just like numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3])\n",
      "torch.Size([5, 4, 3])\n",
      "torch.Size([10, 3])\n"
     ]
    }
   ],
   "source": [
    "# remember, we index elemnts along each dim starting from 0\n",
    "x = torch.rand(10, 4, 3)\n",
    "\n",
    "# first of the 10 elements in the first dimension\n",
    "print(x[0].shape)\n",
    "\n",
    "# colon operator allows you to select everything up to\n",
    "print(x[:5].shape)\n",
    "\n",
    "# colon also lets you select all data along a given dimension\n",
    "# this returns all data along the first dim, and then just the 4th element\n",
    "# thats along the 2nd dim\n",
    "print(x[:, 3].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.1.4 Arithmetic\n",
    "Tensors support the standard broadcasting operations from `numpy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "by broadcasting, x + 10 = tensor([10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10.,\n",
      "        10., 10., 10., 10., 10., 10.])\n"
     ]
    }
   ],
   "source": [
    "# Operations are broadcast across all elements\n",
    "x = torch.zeros(20)\n",
    "x = x+10\n",
    "print('by broadcasting, x + 10 =', x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 times idenity matrix = \n",
      " tensor([[3., 0., 0.],\n",
      "        [0., 3., 0.],\n",
      "        [0., 0., 3.]])\n"
     ]
    }
   ],
   "source": [
    "# This does not depend on the rank or shape of the tensor\n",
    "print('3 times idenity matrix = \\n', 3*I)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.1.5 Data Management\n",
    "By default, tensor elements are stored as floating point and used in the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n",
      "torch.int64\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(3,3)\n",
    "print(x.dtype)\n",
    "\n",
    "# to cast them as 64 bit int\n",
    "print(x.long().dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "CUDA is not available on this device\n"
     ]
    }
   ],
   "source": [
    "# To send the data to the GPU instead\n",
    "x = torch.rand(3,3)\n",
    "\n",
    "# check current device tensor gets sent to\n",
    "print(x.device)\n",
    "if torch.cuda.is_available():\n",
    "    x = x.cuda()\n",
    "    print(x.device)\n",
    "else:\n",
    "    print('CUDA is not available on this device')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If CUDA was avaialble, all subsequent operations on the tensor would run on the GPU and be really quick. `x.cpu()` would bring it back to the cpu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.2 Datasets\n",
    "\n",
    "Data is the bread and butter of all kinds of machine learning. PyTorch comes with several python libraries for different kinds of datasets: `torchvision` for image data, `torchaudio` for audio datasets and `torchtext` for, you guessed it, text based data. We'll explore the basics of `torchvision` here because it follows on quite nicely from tensors we've just covered.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "[1] EigenChris, [Tensors for Beginners 0: Tensor Definition](https://youtu.be/TvxmkZmBa-k)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6ae589f8b689739ef0abe7e46f86a99577497c2d17be5559ea1da817f60b7143"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
